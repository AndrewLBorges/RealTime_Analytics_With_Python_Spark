{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ec04b0",
   "metadata": {},
   "source": [
    "# <font color='blue'>Big Data Real-Time Analytics com Python e Spark</font>\n",
    "\n",
    "## <font color='blue'>Mini-Projeto 6</font>\n",
    "\n",
    "### <font color='blue'>Análise de Dados de Sensores IoT em Tempo Real com Apache Spark Streaming e Apache Kafka</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f835a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.9.13\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e7107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "#!pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "#!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59755c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10830c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6abef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e4f48",
   "metadata": {},
   "source": [
    "> Precisamos incluir o conector de integração do Spark Streaming com o Apache Kafka. Fique atento à versão do PySpark que está sendo usada.\n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31051963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conector\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcb89ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Andrew Borges\n",
      "\n",
      "findspark: 2.0.1\n",
      "pyspark  : 3.3.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Andrew Borges\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e717fec",
   "metadata": {},
   "source": [
    "## Criando a Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f06fbe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Mini-Projeto6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67606d2b",
   "metadata": {},
   "source": [
    "## Leitura do Kafka Spark Structured Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7111c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar uma subscrição no tópico que tem o streaming de dados que desejamos \"puxar\" os dados.\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"dsamp6\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d6dd6",
   "metadata": {},
   "source": [
    "## Definição do Schema da Fonte de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f9f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o schema dos dados que desejamos capturar para análise (temperatura)\n",
    "esquema_dados_temp = StructType([StructField(\"leitura\",\n",
    "                                            StructType([StructField(\"temperatura\", DoubleType(), True)]), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9e6009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o schema global dos dados no streaming\n",
    "esquema_dados = StructType([\n",
    "    StructField(\"id_sensor\", StringType(), True),\n",
    "    StructField(\"id_equipamento\", StringType(), True),\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"data_evento\", StringType(), True),\n",
    "    StructField(\"padrao\", esquema_dados_temp, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc327ebb",
   "metadata": {},
   "source": [
    "## Parse da Fonte de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "767688f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturamos cada linha de dado (cada valor) como string\n",
    "df_conversao = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c43cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse do formato JSON em dataframe\n",
    "df_conversao = df_conversao.withColumn(\"jsonData\", from_json(col(\"value\"), esquema_dados)).select(\"jsonData.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "113bcf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_sensor: string (nullable = true)\n",
      " |-- id_equipamento: string (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- data_evento: string (nullable = true)\n",
      " |-- padrao: struct (nullable = true)\n",
      " |    |-- leitura: struct (nullable = true)\n",
      " |    |    |-- temperatura: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conversao.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a666c",
   "metadata": {},
   "source": [
    "## Preparamos o Dataframe\n",
    "\n",
    "Esse dataframe está no formato que precisamos para análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df24b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "df_conversao_temp_sensor = df_conversao.select(col(\"padrao.leitura.temperatura\").alias(\"temperatura\"),\n",
    "                                              col(\"sensor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7fbb73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temperatura: double (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conversao_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20985c2c",
   "metadata": {},
   "source": [
    "## Análise de Dados em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a7918d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui temos o objeto que irá conter nossa análise, o cálculo da média das temperaturas por sensor\n",
    "df_media_temp_sensor = df_conversao_temp_sensor.groupby(\"sensor\").mean(\"temperatura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24b7729a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- avg(temperatura): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc215086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "df_media_temp_sensor = df_media_temp_sensor.select(col(\"sensor\").alias(\"sensor\"),\n",
    "                                                  col(\"avg(temperatura)\").alias(\"media_temp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce5df2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- media_temp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47e1b4",
   "metadata": {},
   "source": [
    "Abaixo abrimos o streaming para análise de dados em tempo real, imprimindo o resultado no console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "014b59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto que inicia a consulta ao streaming com formato de console\n",
    "query = df_media_temp_sensor.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a58340",
   "metadata": {},
   "source": [
    "Envie novos arquivos para o Kafka a fim de ver a análise em tempo real por aqui. Clique no botão Stop no menu superior para interromper a célula a qualquer momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28e66735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executamos a query do streaming e evitamos que o processo seja encerrado\n",
    "# query.awaitTermination()\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9a7eeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81f140a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0efbd42f-34aa-4e31-baa1-893b5e0ba356',\n",
       " 'runId': '4fca966b-a184-4138-999d-950364ca1631',\n",
       " 'name': None,\n",
       " 'timestamp': '2023-04-17T15:59:14.723Z',\n",
       " 'batchId': 3,\n",
       " 'numInputRows': 0,\n",
       " 'inputRowsPerSecond': 0.0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'latestOffset': 1, 'triggerExecution': 1},\n",
       " 'stateOperators': [{'operatorName': 'stateStoreSave',\n",
       "   'numRowsTotal': 50,\n",
       "   'numRowsUpdated': 0,\n",
       "   'allUpdatesTimeMs': 591,\n",
       "   'numRowsRemoved': 0,\n",
       "   'allRemovalsTimeMs': 0,\n",
       "   'commitTimeMs': 176835,\n",
       "   'memoryUsedBytes': 101688,\n",
       "   'numRowsDroppedByWatermark': 0,\n",
       "   'numShufflePartitions': 200,\n",
       "   'numStateStoreInstances': 200,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 800,\n",
       "    'loadedMapCacheMissCount': 0,\n",
       "    'stateOnCurrentVersionSizeBytes': 30552}}],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[dsamp6]]',\n",
       "   'startOffset': {'dsamp6': {'0': 30000}},\n",
       "   'endOffset': {'dsamp6': {'0': 30000}},\n",
       "   'latestOffset': {'dsamp6': {'0': 30000}},\n",
       "   'numInputRows': 0,\n",
       "   'inputRowsPerSecond': 0.0,\n",
       "   'processedRowsPerSecond': 0.0,\n",
       "   'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
       "    'maxOffsetsBehindLatest': '0',\n",
       "    'minOffsetsBehindLatest': '0'}}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@3e590a1d',\n",
       "  'numOutputRows': 0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cea3ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@4696a783, org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy$$Lambda$2899/0x0000000101349440@5669b645\n",
      "+- *(4) HashAggregate(keys=[sensor#28], functions=[avg(temperatura#36)])\n",
      "   +- StateStoreSave [sensor#28], state info [ checkpoint = file:/C:/Users/andrew/AppData/Local/Temp/temporary-f11d88a4-9539-4cba-886c-0625e84c91e0/state, runId = 4fca966b-a184-4138-999d-950364ca1631, opId = 0, ver = 2, numPartitions = 200], Complete, 0, 2\n",
      "      +- *(3) HashAggregate(keys=[sensor#28], functions=[merge_avg(temperatura#36)])\n",
      "         +- StateStoreRestore [sensor#28], state info [ checkpoint = file:/C:/Users/andrew/AppData/Local/Temp/temporary-f11d88a4-9539-4cba-886c-0625e84c91e0/state, runId = 4fca966b-a184-4138-999d-950364ca1631, opId = 0, ver = 2, numPartitions = 200], 2\n",
      "            +- *(2) HashAggregate(keys=[sensor#28], functions=[merge_avg(temperatura#36)])\n",
      "               +- Exchange hashpartitioning(sensor#28, 200), ENSURE_REQUIREMENTS, [plan_id=1679]\n",
      "                  +- *(1) HashAggregate(keys=[sensor#28], functions=[partial_avg(temperatura#36)])\n",
      "                     +- Project [from_json(StructField(id_sensor,StringType,true), StructField(id_equipamento,StringType,true), StructField(sensor,StringType,true), StructField(data_evento,StringType,true), StructField(padrao,StructType(StructField(leitura,StructType(StructField(temperatura,DoubleType,true)),true)),true), cast(value#8 as string), Some(America/Sao_Paulo)).padrao.leitura.temperatura AS temperatura#36, from_json(StructField(id_sensor,StringType,true), StructField(id_equipamento,StringType,true), StructField(sensor,StringType,true), StructField(data_evento,StringType,true), StructField(padrao,StructType(StructField(leitura,StructType(StructField(temperatura,DoubleType,true)),true)),true), cast(value#8 as string), Some(America/Sao_Paulo)).sensor AS sensor#28]\n",
      "                        +- MicroBatchScan[key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956fbc0b",
   "metadata": {},
   "source": [
    "## Análise de Dados em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3ff3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto que inicia a consulta ao streaming com formato memória (cria tabela temporária)\n",
    "query_memoria = df_media_temp_sensor \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"dsa\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "574721cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x2336e111160>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Streams ativados\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "129d5f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|sensor|media|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|  sensor|media|\n",
      "+--------+-----+\n",
      "| sensor7|78.93|\n",
      "|sensor34|84.57|\n",
      "|sensor41| 62.9|\n",
      "|sensor50|56.88|\n",
      "|sensor38|58.98|\n",
      "|sensor30|73.26|\n",
      "|sensor10| 60.8|\n",
      "| sensor4|76.83|\n",
      "| sensor5|72.55|\n",
      "|sensor19|57.42|\n",
      "| sensor8|52.13|\n",
      "|sensor43|56.17|\n",
      "|sensor47| 51.7|\n",
      "|sensor26|50.42|\n",
      "|sensor12| 53.6|\n",
      "|sensor18|54.65|\n",
      "|sensor15|50.95|\n",
      "| sensor6|64.59|\n",
      "|sensor28|74.03|\n",
      "|sensor33|51.22|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-----+\n",
      "|  sensor|media|\n",
      "+--------+-----+\n",
      "| sensor7|80.54|\n",
      "|sensor34|83.95|\n",
      "|sensor41|63.98|\n",
      "|sensor50|58.18|\n",
      "|sensor38|58.28|\n",
      "|sensor30|72.34|\n",
      "|sensor10|62.15|\n",
      "| sensor4|73.25|\n",
      "| sensor5|71.93|\n",
      "|sensor19|58.15|\n",
      "| sensor8|51.69|\n",
      "|sensor43| 55.0|\n",
      "|sensor47|54.08|\n",
      "|sensor12|53.72|\n",
      "|sensor18|54.85|\n",
      "|sensor15|50.87|\n",
      "| sensor6|63.28|\n",
      "|sensor28|72.16|\n",
      "|sensor11|73.61|\n",
      "|sensor37|63.56|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-----+\n",
      "|  sensor|media|\n",
      "+--------+-----+\n",
      "| sensor7|80.54|\n",
      "|sensor34|83.95|\n",
      "|sensor41|63.98|\n",
      "|sensor50|58.18|\n",
      "|sensor38|58.28|\n",
      "|sensor30|72.34|\n",
      "|sensor10|62.15|\n",
      "| sensor4|73.25|\n",
      "| sensor5|71.93|\n",
      "|sensor19|58.15|\n",
      "| sensor8|51.69|\n",
      "|sensor43| 55.0|\n",
      "|sensor47|54.08|\n",
      "|sensor12|53.72|\n",
      "|sensor18|54.85|\n",
      "|sensor15|50.87|\n",
      "| sensor6|63.28|\n",
      "|sensor28|72.16|\n",
      "|sensor11|73.61|\n",
      "|sensor37|63.56|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos manter a query executando por algum tempo e aplicando SQL aos dados em tempo real\n",
    "from time import sleep\n",
    "\n",
    "for x in range(10):\n",
    "    \n",
    "    spark.sql(\"select sensor, round(media_temp, 2) as media from dsa where media_temp > 50\").show()\n",
    "    sleep(3)\n",
    "    \n",
    "query_memoria.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b5a58",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
